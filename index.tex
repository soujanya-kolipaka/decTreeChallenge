% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrartcl}
\usepackage{xcolor}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother





\setlength{\emergencystretch}{3em} % prevent overfull lines

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



 


\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Decision Tree Challenge},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Decision Tree Challenge}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Feature Importance and Categorical Variable Encoding}
\author{}
\date{}
\begin{document}
\maketitle


\section{ðŸŒ³ Decision Tree Challenge - Feature Importance and Variable
Encoding}\label{decision-tree-challenge---feature-importance-and-variable-encoding}

\subsection{Challenge Overview}\label{challenge-overview}

\textbf{Your Mission:} Create a simple GitHub Pages site that
demonstrates how decision trees measure feature importance and analyzes
the critical differences between categorical and numerical variable
encoding. You'll answer two key discussion questions by adding narrative
to a pre-built analysis and posting those answers to your GitHub Pages
site as a rendered HTML document.

\subsection{The Decision Tree Problem
ðŸŽ¯}\label{the-decision-tree-problem}

\textbf{The Core Problem:} Decision trees are often praised for their
interpretability and ability to handle both numerical and categorical
variables. But what happens when we encode categorical variables as
numbers? How does this affect our understanding of feature importance?

\textbf{What is Feature Importance?} In decision trees, feature
importance measures how much each variable contributes to reducing
impurity (or improving prediction accuracy) across all splits in the
tree. It's a key metric for understanding which variables matter most
for your predictions.

\subsection{ðŸŽ¯ The Key Insight: Encoding Matters for
Interpretability}\label{the-key-insight-encoding-matters-for-interpretability}

\textbf{The problem:} When we encode categorical variables as numerical
values (like 1, 2, 3, 4\ldots), decision trees treat them as if they
have a meaningful numerical order. This can completely distort our
analysis.

\textbf{The Real-World Context:} In real estate, we know that
neighborhood quality, house style, and other categorical factors are
crucial for predicting home prices. But if we encode these as numbers,
we might get misleading insights about which features actually matter
most.

\textbf{The Devastating Reality:} Even sophisticated machine learning
models can give us completely wrong insights about feature importance if
we don't properly encode our variables. A categorical variable that
should be among the most important might appear irrelevant, while a
numerical variable might appear artificially important.

Let's assume we want to predict house prices and understand which
features matter most. The key question is: \textbf{How does encoding
categorical variables as numbers affect our understanding of feature
importance?}

\subsection{The Ames Housing Dataset ðŸ }\label{the-ames-housing-dataset}

We are analyzing the Ames Housing dataset which contains detailed
information about residential properties sold in Ames, Iowa from 2006 to
2010. This dataset is perfect for our analysis because it contains a
categorical variable (like zip code) and numerical variables (like
square footage, year built, number of bedrooms).

\subsection{The Problem: ZipCode as Numerical vs
Categorical}\label{the-problem-zipcode-as-numerical-vs-categorical}

\textbf{Key Question:} What happens when we treat zipCode as a numerical
variable in a decision tree? How does this affect feature importance
interpretation?

\textbf{The Issue:} Zip codes (50010, 50011, 50012, 50013) are
categorical variables representing discrete geographic areas,
i.e.~neighborhoods. When treated as numerical, the tree might split on
``zipCode \textgreater{} 50012.5'' - which has no meaningful
interpretation for house prices. Zip codes are non-ordinal categorical
variables meaning they have no inherent order that aids house price
prediction (i.e.~zip code 99999 is not the priceiest zip code).

\subsection{Data Loading and Model
Building}\label{data-loading-and-model-building}

\phantomsection\label{load-and-model-python}
\begin{verbatim}
Model built with 8 terminal nodes
\end{verbatim}

\subsection{Tree Visualization}\label{tree-visualization}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/figure-pdf/visualize-tree-python-output-1.pdf}}

\subsection{Feature Importance
Analysis}\label{feature-importance-analysis}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/figure-pdf/importance-plot-python-output-1.pdf}}

\subsection{Critical Analysis: The Encoding
Problem}\label{critical-analysis-the-encoding-problem}

\subsection{âš ï¸ The Problem Revealed}\label{the-problem-revealed}

\textbf{What to note:} Our decision tree treated \texttt{zipCode} as a
numerical variable. This leads to zip code being unimportant. Not
surprisingly, because there is no reason to believe allowing splits like
``zipCode \textless{} 50012.5'' should be beneficial for house price
prediction. This false coding of a variable creates several problems:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Potentially Meaningless Splits:} A zip code of 50013 is not
  ``greater than'' 50012 in any meaningful way for house prices
\item
  \textbf{False Importance:} The algorithm assigns importance to zipCode
  based on numerical splits rather than categorical distinctions OR the
  importance of zip code is completely missed as numerical ordering has
  no inherent relationship to house prices.
\item
  \textbf{Misleading Interpretations:} We might conclude zipCode is not
  important when our intuition tells us it should be important (listen
  to your intuition).
\end{enumerate}

\textbf{The Real Issue:} Zip codes are categorical variables
representing discrete geographic areas. The numerical values have no
inherent order or magnitude relationship to house prices. These must be
modelled as categorical variables.

\subsection{Proper Categorical Encoding: The
Solution}\label{proper-categorical-encoding-the-solution}

Now let's repeat the analysis with zipCode properly encoded as
categorical variables to see the difference.

\textbf{Python Approach:} One-hot encode zipCode (create dummy variables
for each zip code)

\subsubsection{Categorical Encoding
Analysis}\label{categorical-encoding-analysis}

\subsubsection{Tree Visualization: Categorical
zipCode}\label{tree-visualization-categorical-zipcode}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/figure-pdf/visualize-tree-cat-python-output-1.pdf}}

\subsubsection{Feature Importance: Categorical
zipCode}\label{feature-importance-categorical-zipcode}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/figure-pdf/importance-plot-cat-python-output-1.pdf}}

\subsection{Discussion Questions for
Challenge}\label{discussion-questions-for-challenge}

\textbf{Your Task:} Add thoughtful narrative answers to these two
questions in the Discussion Questions section of your rendered HTML
site.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Numerical vs Categorical Encoding:} There are two modelsin
  Python written above. For each language, the models differ by how zip
  code is modelled, either as a numerical variable or as a categorical
  variable. Given what you know about zip codes and real estate prices,
  how should zip code be modelled, numerically or categorically? Is
  zipcode and ordinal or non-ordinal variable?
\end{enumerate}

\textbf{Numerical encoding (first model):} - Treats zip codes as if they
were continuous numbers. - The tree could split on rules like zipCode
\textless= 20000. - Problem: Zip codes don't have a meaningful numeric
order --- 19711 isn't ``less expensive'' than 90210 just because the
digits are smaller. - This introduces artificial relationships that
don't exist.

\textbf{Categorical encoding (alternative model):} - Treats each zip
code as a distinct category (like labels). - The model can learn
differences in housing prices across regions without assuming numeric
distance. - This is more appropriate because zip codes represent
locations, not quantities.

\textbf{Conclusion:} For real estate modeling, zip code should be
encoded as categorical, not numerical.

\textbf{Is Zip Code Ordinal or Non-Ordinal?} - Ordinal variable:
Categories have a natural order (e.g., education level: high school
\textless{} bachelor \textless{} master). - Non-ordinal variable:
Categories are labels with no inherent ranking (e.g., colors, zip
codes).

\textbf{Zip codes are non-ordinal.} - They are identifiers for
geographic areas. - The numbering system is arbitrary --- 19711
(Delaware) and 90210 (California) don't imply one is ``greater'' or
``less'' than the other. - There's no natural progression or hierarchy
in the digits themselves.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{R vs Python Implementation Differences:} When modelling zip
  code as a categorical variable, the output tree and feature importance
  would differ quite significantly had you used R as opposed to Python.
  Investigate why this is the case. What does R offer that Python does
  not? Which language would you say does a better job of modelling zip
  code as a categorical variable? Can you quote the documentation at
  \url{https://scikit-learn.org/stable/modules/tree.html} suggesting a
  weakness in the Python implementation? If so, please provide a quote
  from the documentation. Answer: R's decision tree implementations
  (like rpart or CART) natively handle categorical variables, splitting
  them by grouping categories rather than forcing them into numeric
  order. Python's scikit-learn trees do not support categorical features
  directly --- you must manually encode them (e.g., one-hot or ordinal
  encoding). This difference means R often produces more intuitive
  splits and feature importance for categorical variables like zip
  codes. According to scikit-learn's documentation:
  ``DecisionTreeClassifier and DecisionTreeRegressor only support
  numerical features. For categorical features, one-hot encoding is
  required.''
\end{enumerate}

ðŸ”¹ Why R vs Python differ \textbf{R (e.g., rpart):} - Treats categorical
variables as factors. - Splits can be made by grouping categories (e.g.,
zipCode âˆˆ \{19711, 19712\} vs \{90210, 10001\}). - This allows the tree
to directly model categorical distinctions without artificial numeric
ordering. - Feature importance reflects the categorical nature of the
variable, often showing zip code as a strong predictor in housing data.

\textbf{Python (scikit-learn):} - Decision trees only accept numeric
input. - Categorical variables must be encoded manually (e.g., one-hot
encoding). - Encoding changes the way splits are made: One-hot â†’ many
binary columns, feature importance is spread across them. - Ordinal
encoding â†’ imposes a false numeric order, which can distort splits. - As
a result, the output tree and importance measures differ significantly
from R.

\textbf{What R offers that Python does not} - Native categorical
handling: R can split categories directly without preprocessing. -
Cleaner trees: Splits are interpretable (e.g., ``zipCode in \{A, B\} vs
\{C, D\}''), rather than dozens of binary one-hot splits. - Better
feature importance: Importance is attributed to the categorical variable
as a whole, not fragmented across dummy variables.

\textbf{Which language does better?} - For categorical variables like
zip codes, R does a better job because it models them natively and
produces more interpretable trees. - Python can match this with careful
encoding, but it requires extra preprocessing and can lead to less
intuitive results.

\textbf{Documentation quote (scikit-learn weakness)} From the official
scikit-learn decision tree documentation: ``DecisionTreeClassifier and
DecisionTreeRegressor only support numerical features. For categorical
features, one-hot encoding is required.'' This highlights the
limitation: Python's scikit-learn trees cannot directly handle
categorical variables, unlike R.

\textbf{Summary:}

\begin{itemize}
\tightlist
\item
  Zip code should be categorical.
\item
  R natively supports categorical splits, Python does not.
\item
  R produces more intuitive trees and feature importance for categorical
  variables.
\item
  Scikit-learn's documentation explicitly acknowledges this weakness.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \textbf{Are There Any Suggestions for Implementing Decision Trees in
  Python With Prioper Categorical Handling?} Please poke around the
  Internet (AI is not as helpful with new libraries) for suggestions on
  how to implement decision trees in Python with better (i.e.~not
  one-hot encoding) categorical handling. Please provide a link to the
  source and a quote from the source. There is not right answer here,
  but please provide a thoughtful answer, I am curious to see what you
  find.
\end{enumerate}

\textbf{Answer:} \textbf{ðŸŒ³ Decision Trees, Feature Importance, and the
Encoding Story} Decision trees are often celebrated for their
interpretability. They provide a clear ranking of which variables drive
predictions, making them attractive for business users who want
transparency. But when I explored how feature importance is calculated,
I discovered a subtle trap: the way categorical variables are encoded
can dramatically distort the story the tree tells.

\textbf{ðŸ“Š Numerical vs.~Categorical Encoding} Numerical features: Trees
split on thresholds (e.g., ``Age \textless{} 35''), and importance
reflects how often and how effectively those thresholds separate the
data. The rankings are usually intuitive and stable.

Categorical features with one-hot encoding: Instead of one coherent
variable, the tree sees dozens of binary flags. This can fragment
importance across many dummy variables or inflate it for
high-cardinality categories (like ZIP codes). The result: importance
rankings that look impressive but may mislead decision-makers.

This isn't just theory---the official documentation for scikit-learn
makes the limitation explicit:

``DecisionTreeRegressor does not support categorical variables. You need
to encode them numerically, e.g.~with one-hot encoding.'' Scikit-learn
DecisionTreeRegressor Documentation

\textbf{ðŸš€ State of the Art in Python} Modern libraries have moved
beyond one-hot encoding:

\textbf{LightGBM:} Uses integer encoding and statistical tests to find
optimal splits across categories, avoiding the dimensionality explosion
of one-hot.

\textbf{CatBoost:} Designed specifically for categorical features,
applying target statistics with regularization to produce more
meaningful importance scores.

Both approaches keep categorical variables intact, so importance
reflects the true predictive power of the feature rather than artifacts
of encoding.

\textbf{ðŸ’¡ Human Insights} Feature importance is not absolute
truth---it's a lens.

With one-hot encoding, the lens is distorted: importance rankings may
exaggerate or fragment categorical variables.

With native categorical handling, the lens sharpens: importance scores
align more closely with business intuition, showing which variables
genuinely matter.

\textbf{âœ… Concise Conclusions} Decision trees remain powerful for
interpretability, but encoding choices can make or break the insights.

Scikit-learn requires one-hot encoding, which can mislead feature
importance analysis.

Libraries like LightGBM and CatBoost represent the current state of the
art, offering native categorical handling and more trustworthy
importance rankings.

\textbf{ðŸ‘‰ The practical takeaway: if your business relies on
categorical-heavy data (customer segments, product categories, regions),
choose a library that respects categories natively. It's the difference
between a clear story and a distorted one.}




\end{document}
